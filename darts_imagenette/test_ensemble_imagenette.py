# -*- coding: utf-8 -*-
"""test_ensemble_imagenette.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMlNUwMI9KvGZW0CWERa_JHO0E84oPUJ
"""

path_logits_darts = path_logits + "cnn/logits.p"
path_logits1 = path_logits + "cnn_space1/logits.p"
path_logits2 = path_logits + "cnn_space2/logits.p"
path_logits3 = path_logits + "cnn_space3/logits.p"

import os
import pickle
import sys
import numpy as np
from torch.autograd import Variable
import time
import torch
import utils
import glob
import random
import logging
import argparse
import torch.nn as nn
import torch.utils
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.backends.cudnn as cudnn

class CrossEntropyLabelSmooth(nn.Module):

  def __init__(self, num_classes, epsilon):
    super(CrossEntropyLabelSmooth, self).__init__()
    self.num_classes = num_classes
    self.epsilon = epsilon
    self.logsoftmax = nn.LogSoftmax(dim=1)

  def forward(self, inputs, targets):
    log_probs = self.logsoftmax(inputs)
    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes
    loss = (-targets * log_probs).mean(0).sum()
    return loss

criterion = nn.CrossEntropyLoss()
criterion = criterion.cuda()
criterion_smooth = CrossEntropyLabelSmooth(10, 0.1)
criterion_smooth = criterion_smooth.cuda()

validdir = os.path.join(data, 'val')
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

valid_data = dset.ImageFolder(
    validdir,
    transforms.Compose([
      transforms.Resize(256),
      transforms.CenterCrop(224),
      transforms.ToTensor(),
      normalize,
    ]))

valid_queue = torch.utils.data.DataLoader(
    valid_data, batch_size=128, shuffle=False, pin_memory=True, num_workers=4)

import copy
logits = []
logits1 = pickle.load( open( path_logits1 ,"rb" ) )
logits2 = pickle.load( open( path_logits2 ,"rb" ) )
logits3 = pickle.load( open( path_logits3 ,"rb" ) )
logits_darts = pickle.load( open( path_logits_darts ,"rb" ) )
logits = copy.deepcopy(logits1)
for i in range(len(logits)):
    for j in range(len(logits[i])):
      logits[i][j] = logits[i][j] + logits2[i][j] + logits3[i][j]

predicted_darts = []
for out in logits_darts:
  _, predicted = torch.max(out.data, 1)
  preds = predicted.cpu().detach().numpy()
  for pred in preds:
    predicted_darts.append(pred)

predicted_all = []
for out in logits:
  _, predicted = torch.max(out.data, 1)
  preds = predicted.cpu().detach().numpy()
  for pred in preds:
    predicted_all.append(pred)

predicted_space1 = []
for out in logits1:
  _, predicted = torch.max(out.data, 1)
  preds = predicted.cpu().detach().numpy()
  for pred in preds:
    predicted_space1.append(pred)

predicted_space2 = []
for out in logits2:
  _, predicted = torch.max(out.data, 1)
  preds = predicted.cpu().detach().numpy()
  for pred in preds:
    predicted_space2.append(pred)

predicted_space3 = []
for out in logits3:
  _, predicted = torch.max(out.data, 1)
  preds = predicted.cpu().detach().numpy()
  for pred in preds:
    predicted_space3.append(pred)

targets_all = []
for step, (input, target) in enumerate(valid_queue):
    input = Variable(input, volatile=True).cuda()
    targets = Variable(target, volatile=True).cuda(async=True)
    targets = targets.cpu().detach().numpy()
    targets_all.extend(targets)
    

print(len(targets_all))

print(len(predicted_all))

from sklearn.metrics import accuracy_score
print(100*accuracy_score(predicted_darts,targets_all))
print(100*accuracy_score(predicted_all,targets_all))
print(100*accuracy_score(predicted_space1,targets_all))
print(100*accuracy_score(predicted_space2,targets_all))
print(100*accuracy_score(predicted_space3,targets_all))

